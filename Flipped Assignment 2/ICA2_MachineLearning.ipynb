{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Ebnable HTML/CSS \n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<link href='https://fonts.googleapis.com/css?family=Passion+One' rel='stylesheet' type='text/css'><style>div.attn { font-family: 'Helvetica Neue'; font-size: 30px; line-height: 40px; color: #FFFFFF; text-align: center; margin: 30px 0; border-width: 10px 0; border-style: solid; border-color: #5AAAAA; padding: 30px 0; background-color: #DDDDFF; }hr { border: 0; background-color: #ffffff; border-top: 1px solid black; }hr.major { border-top: 10px solid #5AAA5A; }hr.minor { border: none; background-color: #ffffff; border-top: 5px dotted #CC3333; }div.bubble { width: 65%; padding: 20px; background: #DDDDDD; border-radius: 15px; margin: 0 auto; font-style: italic; color: #f00; }em { color: #AAA; }div.c1{visibility:hidden;margin:0;height:0;}div.note{color:red;}</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Enter Team Member Names here (*double click to edit*):\n",
    "\n",
    "- Name 1:\n",
    "- Name 2:\n",
    "- Name 3:\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Class Assignment Two\n",
    "In the following assignment you will be asked to fill in python code and derivations for a number of different problems. Please read all instructions carefully and turn in the rendered notebook (or HTML of the rendered notebook)  before the end of class (or right after class). The initial portion of this notebook is given before class and the remainder is given during class. Please answer the initial questions before class, to the best of your ability. Once class has started you may rework your answers as a team for the initial part of the assignment. \n",
    "\n",
    "<a id=\"top\"></a>\n",
    "## Contents\n",
    "* <a href=\"#Loading\">Loading the Data</a>\n",
    "* <a href=\"#ff\">Defining a Feedforward Network in Python</a>\n",
    "* <a href=\"#bp\">Back Propagation in Python</a>\n",
    "* <a href=\"#vis\">Visualizing Back Propagation</a>\n",
    "________________________________________________________________________________________________________\n",
    "\n",
    "<a id=\"Loading\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Loading the Data\n",
    "Please run the following code to read in the \"digits\" dataset from sklearn's data loading module. This data contains hand written digits for the characters 0-9.\n",
    "\n",
    "This will load the data into the variable `ds`. `ds` is a `bunch` object with fields like `ds.data` and `ds.target`. The field `ds.data` is a numpy matrix of the continuous features in the dataset. **The object is not a pandas dataframe. It is a numpy matrix.** Each row is a set of observed instances, each column is a different feature. It also has a field called `ds.target` that is an integer value we are trying to predict (i.e., a specific integer represents a specific person). Each entry in `ds.target` is a label for each row of the `ds.data` matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load up the handwritten digit dataset\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "ds = load_digits()\n",
    "X = ds.data/16.0-0.5 # normalize the data\n",
    "y = ds.target\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(np.min(X),np.max(X))\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape and print a few of the images in the digits dataset\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True,)\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = X[i].reshape(8, 8)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "____\n",
    "**Question 1:** For the digits dataset, what does each column in $\\mathbf{X}$ represent? What does each row in $\\mathbf{X}$ represent? What does each value in $\\mathbf{X}$ represent? What does each unique value of the target, $y$ represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter you answer here (*double click to edit*)\n",
    "\n",
    "- Column:\n",
    "- Row: \n",
    "- Value:\n",
    "- Unique Target in $y$:\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/eclarson/MachineLearningNotebooks/master/PDF_Slides/MultiLayerNetwork.png\" width=\"500\">\n",
    "\n",
    "**Question 2:** For the digits dataset, we want to train a neural network with one hidden layer (two layers total). The hidden layer will have 30 neurons. What will be the size of the matrices and bias terms in each layer? That is, what is the size of $\\mathbf{W}^{(1)}$, $\\mathbf{b}^{(1)}$, and what is the size of $\\mathbf{W}^{(2)}$, $\\mathbf{b}^{(2)}$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Enter you answer here (*double click to edit*)\n",
    "\n",
    "- $\\mathbf{W}^{(1)}$:\n",
    "- $\\mathbf{b}^{(1)}$:\n",
    "- $\\mathbf{W}^{(2)}$:\n",
    "- $\\mathbf{b}^{(2)}$:\n",
    "\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ff\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "\n",
    "\n",
    "# Defining a Feedforward Network\n",
    "\n",
    "Below we will setup the functions for use in a feedforward neural network **with two layers**. Take a quick look at the functions defined. There are a number of convenience functions including:\n",
    "- a function for the sigmoid calculation\n",
    "- a function to one hot encode the output\n",
    "- an initialization function for initializing the weights\n",
    "\n",
    "A few functions are not yet implemented including:\n",
    "- a `fit` function\n",
    "- a `get_gradient` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "# This is the optional book we use in the course, excellent intuitions and straightforward programming examples\n",
    "# please note, however, that this code has been manipulated to reflect our assumptions and notation.\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted\n",
    "# it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_) * np.sqrt(np.mean(W1 ** 2) + np.mean(W2 ** 2))\n",
    "    \n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0, size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_) # reshape to be W\n",
    "        b1 = np.zeros((self.n_hidden, 1))\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden)\n",
    "        b2 = np.zeros((self.n_output_, 1))\n",
    "        \n",
    "        return W1, W2, b1, b2\n",
    "    \n",
    "    \n",
    "    def _feedforward(self, X, W1, W2, b1, b2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        -----------\n",
    "        X : Input layer with original features.\n",
    "        W1: Weight matrix for input layer -> hidden layer.\n",
    "        W2: Weight matrix for hidden layer -> output layer.\n",
    "        ----------\n",
    "        a1-a3 : activations into layer (or output layer)\n",
    "        z1-z2 : layer inputs \n",
    "\n",
    "        \"\"\"\n",
    "        A1 = X.T\n",
    "        Z1 = W1 @ A1 + b1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        Z2 = W2 @ A2 + b2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2, self.b1, self.b2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # will write this function below\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\n",
    "            Input X should be a matrix with separate instances\n",
    "            in each row, and separate features in each column.\n",
    "            The target variable, y, should be integer values \n",
    "            starting from zero, that represent the unique classes.\n",
    "        \"\"\"\n",
    "        # will write this function below\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<img src=\"https://raw.githubusercontent.com/eclarson/MachineLearningNotebooks/master/PDF_Slides/MultiLayerNetwork.png\" width=\"500\">\n",
    "\n",
    "Recall from the videos that all instances in $\\mathbf{X}$ can be fed into the network with a single matrix multiplication operation for each intermediate vector, $\\mathbf{a}^{(l)}$ and $\\mathbf{z}^{(l)}$. When we feed all the instances, $\\mathbf{X}$, the intermediate vectors, $\\mathbf{a}^{(l)}$ and $\\mathbf{z}^{(l)}$ get stacked together to form matrices, $\\mathbf{A}^{(l)}$ and $\\mathbf{Z}^{(l)}$. This is already done for you in the `_feedforward` function defined above.\n",
    "\n",
    "**Question 3:**\n",
    "For the digits dataset we are using and a network with 30 neurons in the hidden layer, what are the sizes of:\n",
    "- **Part A**: the intermediate vectors, $\\mathbf{a}^{(1)}$ and $\\mathbf{a}^{(2)}$\n",
    "- **Part B**: the intermediate vectors, $\\mathbf{z}^{(1)}$ and $\\mathbf{z}^{(2)}$\n",
    "- **Part C**: the intermediate matrices, $\\mathbf{A}^{(1)}$ and $\\mathbf{A}^{(2)}$\n",
    "- **Part D**: the intermediate matrices, $\\mathbf{Z}^{(1)}$ and $\\mathbf{Z}^{(2)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter you answer here (double click)*\n",
    "\n",
    "\n",
    "A. \n",
    "\n",
    "\n",
    "B.  \n",
    "\n",
    "\n",
    "C. \n",
    "\n",
    "\n",
    "D.  \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"bp\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "# Back Propagation in Python\n",
    "Now let's add in the back propagation steps from the video. First, we need to add in a `fit` function that will update all the trainable weights in the $\\mathbf{W}^{(l)}$ matrices. Because this is a two layer network we have layers $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$. Look at the given `fit` function written below for you. \n",
    "\n",
    "The `fit` function below will update matrices using steepest descent. And the update equation is:\n",
    "$$  w_{i,j}^{(l)} \\leftarrow w_{i,j}^{(l)} - \\eta \\frac{\\partial J(\\mathbf{W})}{\\partial w_{i,j}^{(l)}}$$\n",
    "\n",
    "for each value in each matrix, $\\mathbf{W}^{(l)}$. \n",
    "\n",
    "The objective function is simply the mean squared error:\n",
    "$$ J(\\mathbf{W}) = \\sum_{k=1}^M (\\mathbf{y}^{(k)}-[\\mathbf{a}^{(L)}]^{(k)})^2 $$\n",
    "\n",
    "where $L$ is the output of the last layer. For our two layer implementation, $L=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptron(TwoLayerPerceptronBase):\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\n",
    "            Input X should be a matrix with separate instances\n",
    "            in each row, and separate features in each column.\n",
    "            The target variable, y, should be integer values \n",
    "            starting from zero, that represent the unique classes.\n",
    "        \"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "        \n",
    "        #------------------------------------------\n",
    "        # You will update These arrays, initialized here \n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        \n",
    "        #------------------------------------------\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X_data,self.W1,self.W2, self.b1, self.b2)\n",
    "            \n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            # WE HAVE NOT YET WRITTEN THE GRADIENT FUNCTION YET\n",
    "            gradW1, gradW2, gradb1, gradb2 = self._get_gradient(A1=A1, \n",
    "                                                                A2=A2, \n",
    "                                                                A3=A3, \n",
    "                                                                Z1=Z1, \n",
    "                                                                Z2=Z2, \n",
    "                                                                Y_enc=Y_enc,\n",
    "                                              W1=self.W1, W2=self.W2)\n",
    "            \n",
    "            #------------------------------------------\n",
    "            # ENTER YOUR CODE HERE to keep track of the \n",
    "            # magnitude of gradients grad_w1_ and grad_w2_\n",
    "            \n",
    "            # Track the magnitude of the gradient \n",
    "            #self.grad_w1_[i] = ???\n",
    "            #self.grad_w2_[i] = ???\n",
    "            \n",
    "            #------------------------------------------\n",
    "\n",
    "            self.W1 -= self.eta * gradW1\n",
    "            self.W2 -= self.eta * gradW2\n",
    "            self.b1 -= self.eta * gradb1\n",
    "            self.b2 -= self.eta * gradb2\n",
    "            \n",
    "\n",
    "        return self\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** In the code above, add class properties (variables), `grad_w1_` and `grad_w2_` that save the average magnitude of the gradient for each layer at every epoch. That is, if you ran the `fit` function for 50 epochs, `grad_w1_` and `grad_w2_` would be 50 element vectors when training is complete.\n",
    "\n",
    "When training is completed, `grad_w1_` and `grad_w2_` should be accessible using dot notation from the class object, as shown in the example syntax below. \n",
    "\n",
    "```\n",
    "clf = TwoLayerPerceptron()\n",
    "clf.fit(X,y)\n",
    "clf.grad_w1_ \n",
    "clf.grad_w2_\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the Gradient Calculation\n",
    "From the videos, recall that the the sensitivities, $\\mathbf{V}$, can be calculated as follows:\n",
    "\n",
    "$$ \\mathbf{V}^{(2)} = -2(\\mathbf{Y}-\\mathbf{A}^{(3)})*\\mathbf{A}^{(3)}*(1-\\mathbf{A}^{(3)}) $$\n",
    "$$ \\mathbf{V}^{(1)} =  \\mathbf{A}^{(2)} * (1-\\mathbf{A}^{(2)}) * [\\mathbf{W}^{(2)}]^T \\cdot \\mathbf{V}^{(2)}  $$\n",
    "\n",
    "Once we have these sensitivities, its easy to calculate the gradient of each layer. The gradient of the objective function with respect to the final layer $\\mathbf{W}^{(2)}$ can be calculated with:\n",
    "$$  \\frac{\\nabla J(\\mathbf{W})}{\\partial\\mathbf{W}^{(2)}} = \\mathbf{V}^{(2)}\\cdot [\\mathbf{A}^{(2)}]^T $$\n",
    "\n",
    "And the gradient of the objective function with respect to the first layer $\\mathbf{W}^{(2)}$ can be calculated with: \n",
    "$$  \\frac{\\nabla J(\\mathbf{W})}{\\partial\\mathbf{W}^{(1)}} = \\mathbf{V}^{(1)}\\cdot [\\mathbf{A}^{(1)}]^T $$\n",
    "\n",
    "___\n",
    "** Exercise 2:** In the code below, use numpy linear algebra functions to calculate the sensitivities at each layer, $\\mathbf{V}^{(1)}$ and $\\mathbf{V}^{(2)}$. This will complete the `_get_gradient` private method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptronVect(TwoLayerPerceptron):\n",
    "    # just need a different gradient calculation\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        \n",
    "        #---------------------------------\n",
    "        # ENTER YOUR CODE HERE to\n",
    "        # calculate the sensitivities\n",
    "        # NOTE: be sure to use parentheses for correct order of operations...\n",
    "        V2 = \n",
    "        V1 =   \n",
    "        #---------------------------------\n",
    "        \n",
    "        # Use of sensitivity is calculated for you here\n",
    "        gradW2 = V2 @ A2.T\n",
    "        gradW1 = V1 @ A1.T\n",
    "        \n",
    "        gradb2 = np.sum(V2, axis=1).reshape((-1,1))\n",
    "        gradb1 = np.sum(V1, axis=1).reshape((-1,1))\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        gradW1 += W1 * self.l2_C * 2\n",
    "        gradW2 += W2 * self.l2_C * 2 \n",
    "\n",
    "        return gradW1, gradW2, gradb1, gradb2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"vis\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "\n",
    "# Visualizing Back Propagation\n",
    "Now let's test the code you wrote above to perform gradient calculations and save the magnitude of the gradient at each epoch. Run the code in the cell below. If it runs and the accuracy is relatively high, it is likely that your sensitivity calculation above was written correctly.\n",
    "\n",
    "*If it does not run or the accuracy is low, there is probably something wrong with the sensitivity calculation. Try to fix it before moving on.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TwoLayerPerceptronVect(n_hidden=10, epochs=1500, eta=0.001)\n",
    "clf.fit(X,y)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "yhat = clf.predict(X)\n",
    "accuracy_score(y,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Visualizing the gradients\n",
    "Now let's check your calculation of the average gradient magnitude. Run the code below to visualize the average gradient magnitude versus the epochs run while training. \n",
    "\n",
    "*If the code below does not run, you likely have an error in your calculation of the average gradient magnitude.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(1,1,1)\n",
    "plt.plot(np.abs(clf.grad_w1_[10:]), label='w1')\n",
    "plt.plot(np.abs(clf.grad_w2_[10:]), label='w2')\n",
    "plt.legend()\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** In the plot above, you should see that one layer always has a larger gradient magnitude than the other layer. \n",
    "\n",
    "- **Part A:** Which layer always has the larger magnitude? Why do you think this particular layer always has a larger magnitude? \n",
    "- **Part B:** If one layer has a larger gradient, does this also mean that the weights for that layer are training more quickly (that is, with fewer iterations)? Is that desireable for training the neural network? Explain your reasoning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter you answer here (double click to edit)*\n",
    "\n",
    "- Part A: \n",
    "- Part B: \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Exercise:** In the block of code below, come up with an adaptive scheme to mitigate the effect of unequal magnitude gradients in each layer. Your algorithm should not 'break' the optimization algorithm (*i.e.*, the accuracy should remain somewhat high). Save the magnitude of the resulting update for each layer. \n",
    "\n",
    "Explain your scheme below (that is, document what your adaptive algorithm does) and then implement your strategy below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Enter description here (*double click to edit*)\n",
    "\n",
    "For preventing unequal gradients, we decided to ...\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptronNew(TwoLayerPerceptronVect):\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\n",
    "            Input X should be a matrix with separate instances\n",
    "            in each row, and separate features in each column.\n",
    "            The target variable, y, should be integer values \n",
    "            starting from zero, that represent the unique classes.\n",
    "        \"\"\"\n",
    "        \n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2, self.b1, self.b2 = self._initialize_weights()\n",
    "        \n",
    "        #------------------------------------------\n",
    "        # You will update These arrays, initialized here \n",
    "        self.grad_w1_ = np.zeros(self.epochs)\n",
    "        self.grad_w2_ = np.zeros(self.epochs)\n",
    "        self.update_w1_ = np.zeros(self.epochs)\n",
    "        self.update_w2_ = np.zeros(self.epochs)\n",
    "        \n",
    "        #------------------------------------------\n",
    "\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # feedforward all instances\n",
    "            A1, Z1, A2, Z2, A3 = self._feedforward(X_data,self.W1,self.W2, self.b1, self.b2)\n",
    "            \n",
    "            cost = self._cost(A3,Y_enc,self.W1,self.W2)\n",
    "            self.cost_.append(cost)\n",
    "\n",
    "            # compute gradient via backpropagation\n",
    "            gradW1, gradW2, gradb1, gradb2 = self._get_gradient(A1=A1, \n",
    "                                                                A2=A2, \n",
    "                                                                A3=A3, \n",
    "                                                                Z1=Z1, \n",
    "                                                                Z2=Z2, \n",
    "                                                                Y_enc=Y_enc,\n",
    "                                              W1=self.W1, W2=self.W2)\n",
    "            \n",
    "            #------------------------------------------\n",
    "            # ENTER YOUR CODE HERE to keep track of the \n",
    "            #    average magnitude of gradient of each layer \n",
    "            #    grad_w1_ and grad_w2_\n",
    "            #    and make the eta values for each adaptive \n",
    "            \n",
    "            \n",
    "            # Track the magnitude of the gradient \n",
    "            #self.grad_w1_[i] = ??? #(use same calculation as above)\n",
    "            #self.grad_w2_[i] = ??? #(use same calculation as above)\n",
    "            \n",
    "            # eta1 = ??? #(new dynamic eta values)\n",
    "            # eta2 = ??? #(how should these be calculated???)\n",
    "            \n",
    "            #------------------------------------------\n",
    "\n",
    "            self.W1 -= self.eta * gradW1\n",
    "            self.W2 -= self.eta * gradW2\n",
    "            self.b1 -= self.eta * gradb1\n",
    "            self.b2 -= self.eta * gradb2\n",
    "            \n",
    "            #------------------------------------------\n",
    "            # ENTER YOUR CODE HERE\n",
    "            # Track the magnitude of the gradient update here\n",
    "            # This should be AFTER applying your dynamic scaling\n",
    "            # That is, you SHOULD include eta here.\n",
    "            \n",
    "            #self.update_w1_[i] = ???\n",
    "            #self.update_w2_[i] = ???\n",
    "            \n",
    "            #------------------------------------------\n",
    "            \n",
    "\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    \n",
    "clf2 = TwoLayerPerceptronNew(n_hidden=10, epochs=1500, eta=0.001)\n",
    "clf2.fit(X,y)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "yhat = clf2.predict(X)\n",
    "# the accuracy of the classifier should remain high!\n",
    "# Do not let your dynamic updates make the classifier worse...\n",
    "print(accuracy_score(y,yhat))\n",
    "\n",
    "ax = plt.subplot(1,1,1)\n",
    "plt.plot(np.abs(clf2.update_w1_[10:]), label='w1')\n",
    "plt.plot(np.abs(clf2.update_w2_[10:]), label='w2')\n",
    "plt.legend()\n",
    "plt.ylabel('Average gradient magnitude')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "\n",
    "That's all! Please **save (make sure you saved!!!) and upload your rendered notebook** and please include **team member names** in the notebook submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
