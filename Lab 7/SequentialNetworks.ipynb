{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Sequential Network Architectures\n",
    "\n",
    "## by Michael Doherty, Leilani Guzman, and Carson Pittman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We've been trying to reach you about your car's extended warranty.\" There are few things more annoying in life than answering your phone and hearing these words. With technology being readily available to many people throughout the world, these types of scams have become much more prevalent. These \"spam\" messages, while not always scams, rarely add anything to society.\n",
    "\n",
    "While advancing technology has allowed spam messages to increase in numbers over the years, it has also allowed for new ways to combat these messages. These spam detection filters can automatically block any incoming calls, emails, texts, etc. that appear to be suspicious; however, these filters need to be fairly refined, as blocking too many legitimate messages would lead to distrust in the filter's reliability.\n",
    "\n",
    "Our dataset, titled \"SMS Spam Collection Dataset\", is comprised of over 5000 text messages in English, with each being tagged as either <code>spam</code> or <code>ham</code> (which means it's a legitimate message). Our task is to create a Sequential Network that can classify text messages as either <code>spam</code> or <code>ham</code>.\n",
    "\n",
    "Link to the dataset: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset\n",
    "\n",
    "## 1. Preparation\n",
    "### 1.1 Preprocessing and Tokenization\n",
    "To start, we'll first read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   v1          5572 non-null   object\n",
      " 1   v2          5572 non-null   object\n",
      " 2   Unnamed: 2  50 non-null     object\n",
      " 3   Unnamed: 3  12 non-null     object\n",
      " 4   Unnamed: 4  6 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/spam.csv\", encoding='ISO-8859-1')\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the dataset seems to include some useless columns... Let's go ahead and remove those. We'll also rename our remaining columns so their purpose is clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Label   5572 non-null   object\n",
      " 1   Text    5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(labels=[\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1, inplace=True)\n",
    "\n",
    "df.rename(columns={'v1': 'Label', 'v2': 'Text'}, inplace=True)\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary:\n",
      "8920\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10)\n",
    "tokenizer.fit_on_texts(df[\"Text\"])\n",
    "\n",
    "print(\"\\nVocabulary:\")\n",
    "print(len(tokenizer.word_index))\n",
    "# VOCAB_SIZE = len(tokenizer.word_index) \n",
    "# sequences = tokenizer.texts_to_sequences(df[\"Text\"])\n",
    "\n",
    "# padded_sequences = pad_sequences(sequences)\n",
    "max_review_length = 500\n",
    "X = tokenizer.texts_to_sequences(df[\"Text\"])\n",
    "X = pad_sequences(X, padding='post')\n",
    "y = df[\"Label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Performance Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "F1 score?? We want to block as much spam as possible, but we also don't want to block legitimate texts (as that would probably be even worse than not blocking spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Training and Testing Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "StratifiedKFold, basically the same as Lab 5 (as the dataset is imbalanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "def get_f1_score(X, y, new_model):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    f1_scores = []\n",
    "    mean_tpr_list = []\n",
    "    auc_list = []\n",
    "\n",
    "    i = 1\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        X_train = pad_sequences(X_train, maxlen=max_review_length)\n",
    "        X_test = pad_sequences(X_test, maxlen=max_review_length)\n",
    "        print('Type of X_train:', type(X_train))\n",
    "        print('Type of X_test:', type(X_test))\n",
    "        print('Type of y_train:', type(y_train))\n",
    "        print('Type of y_test:', type(y_test))\n",
    "\n",
    "        VOCAB_SIZE = X_train.shape[1]\n",
    "        print(VOCAB_SIZE)\n",
    "        history = new_model.fit(X_train, y_train, batch_size=64,\n",
    "                                epochs=5, verbose=0,\n",
    "                                validation_data=(X_test,y_test),\n",
    "                                callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
    "        print('after fit')\n",
    "        print(f\"Fold {i}\")\n",
    "        i += 1\n",
    "        \n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(history.history['f1_score'], label='training')\n",
    "\n",
    "        plt.ylabel('F1 Score %')\n",
    "        plt.title('Training')\n",
    "        plt.plot(history.history['val_f1_score'], label='validation')\n",
    "        plt.title('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(history.history['loss'], label='training')\n",
    "        plt.ylabel('Training Loss')\n",
    "        plt.xlabel('epochs')\n",
    "\n",
    "        plt.plot(history.history['val_loss'], label='validation')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.title('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        f1_scores.append(history.history['val_f1_score'][-1].numpy())\n",
    "        print('HEREEEEEEEE')\n",
    "        yhat = new_model.predict(X_test, verbose=0)\n",
    "        yhat = tf.round(yhat).numpy()\n",
    "        print(type(yhat))\n",
    "\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(y_test, yhat.numpy())\n",
    "        mean_tpr_list.append(np.interp(mean_fpr, fpr, tpr))\n",
    "        auc_list.append(auc(fpr, tpr))\n",
    "\n",
    "    plt.bar(range(len(f1_scores)), f1_scores)\n",
    "    plt.ylim([min(f1_scores) - 0.01, max(f1_scores)])\n",
    "    plt.title('Validation F1 Score')\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('F1 Score')\n",
    "    print(\"Average F1 Score:\", np.mean(f1_scores))\n",
    "    \n",
    "    return f1_scores, mean_tpr_list, auc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling\n",
    "\n",
    "### 2.1 Model Creation\n",
    "\n",
    "#### 2.1.1 Model 1: CNN Sequential Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate\n",
    "from tensorflow.keras.layers import Subtract\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "NUM_CLASSES = 1\n",
    "EMBED_SIZE = 50\n",
    "# The input is a list of integers, 500 long\n",
    "# NEED TO DEFINE X_train\n",
    "sequence_input = Input(shape=(VOCAB_SIZE,), dtype='int32')\n",
    "\n",
    "# this will reduce the input dimension from VOCAB_SIZE to 50 for each word\n",
    "# the lenght will be the maximum number of words in a document, so 500\n",
    "embedded_sequences = Embedding(VOCAB_SIZE,\n",
    "                                EMBED_SIZE, # output dimension size\n",
    "                                input_length=max_review_length)(sequence_input) # number of words in each sequence\n",
    "\n",
    "# Starting sequence size is 500 (words) by 50 (embedded features)\n",
    "x = Conv1D(64, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(embedded_sequences)\n",
    "\n",
    "# After conv, length becomes: 500-4=496\n",
    "# So overall size is 496 by 64\n",
    "\n",
    "# Now pool across time\n",
    "x = MaxPooling1D(5)(x)# after max pool, 496/5 -> 99 by 64\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Extract additional features\n",
    "x = Conv1D(64, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(x)\n",
    "\n",
    "# New size is 95 after the conovlutions\n",
    "x = MaxPooling1D(5)(x) # after max pool, size is 95/5 = 19 by 64\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# More features through CNN processing\n",
    "x = Conv1D(64, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# After convolution, size becomes 15 elements long\n",
    "# Take the mean of these elements across features, result is 64 elements\n",
    "x_mean = GlobalAveragePooling1D()(x) # this is the size to globally flatten\n",
    "# Take the variance of these elements across features, result is 64 elements\n",
    "x_tmp = Subtract()([x,x_mean])\n",
    "x_std = GlobalAveragePooling1D()(x_tmp**2)\n",
    "\n",
    "x = Concatenate(name='concat_1')([x_mean,x_std])\n",
    "\n",
    "x = Dense(64, activation='relu',\n",
    "          kernel_initializer='he_uniform')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "predictions = Dense(NUM_CLASSES, activation='sigmoid',\n",
    "              kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "\n",
    "cnn = Model(sequence_input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "cnn.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[f1_score])\n",
    "\n",
    "print(type(X))\n",
    "y = np.array(y)\n",
    "print(type(y))\n",
    "f1_scores, mean_tpr_list, auc_list = get_f1_score(X, y, cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Summarize history for f1 score\n",
    "plt.plot(history[0].history['f1_score'])\n",
    "plt.plot(history[0].history['val_f1_score'])\n",
    "plt.title('CNN f1 score')\n",
    "plt.ylabel('f1 score')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# Summarize history for loss\n",
    "plt.plot(history[0].history['loss'])\n",
    "plt.plot(history[0].history['val_loss'])\n",
    "plt.title('CNN loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Model 2: Transformer Sequential Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "\n",
    "# The transformer architecture\n",
    "class TransformerBlock(Layer): # inherit from Keras Layer\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.2):\n",
    "        super().__init__()\n",
    "        # Setup the model heads and feedforward network\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embed_dim)\n",
    "       \n",
    "        # Make a two layer network that processes the attention\n",
    "        self.ffn = Sequential()\n",
    "        self.ffn.add( Dense(ff_dim, activation='relu') )\n",
    "        self.ffn.add( Dense(embed_dim) )\n",
    "       \n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Apply the layers as needed\n",
    "        # Get the attention output from multi heads\n",
    "        # Using same inpout here is self-attention\n",
    "        # Call inputs are (query, value, key)\n",
    "        # If only two inputs given, value and key are assumed the same\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "       \n",
    "        # Create residual output, with attention\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "       \n",
    "        # Apply dropout if training\n",
    "        out1 = self.dropout1(out1, training=training)\n",
    "       \n",
    "        # Place through feed forward after layer norm\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "       \n",
    "        # Apply dropout if training\n",
    "        out2 = self.dropout2(out2, training=training)\n",
    "        #return the residual from Dense layer\n",
    "        return out2\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        # create two embeddings\n",
    "        # one for processing the tokens (words)\n",
    "        self.token_emb = Embedding(input_dim=vocab_size,\n",
    "                                   output_dim=embed_dim)\n",
    "        # another embedding for processing the position\n",
    "        self.pos_emb = Embedding(input_dim=maxlen,\n",
    "                                 output_dim=embed_dim)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        # create a static position measure (input)\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        # positions now goes from 0 to 500 by 1\n",
    "        positions = self.pos_emb(positions)# embed these positions\n",
    "        x = self.token_emb(x) # embed the tokens\n",
    "        return x + positions # add embeddngs to get final embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "x = TokenAndPositionEmbedding(X_train.shape[1], VOCAB_SIZE, embed_dim)(inputs)\n",
    "x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(20, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(NUM_CLASSES, activation='sigmoid',\n",
    "              kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "\n",
    "xformer = Model(inputs=inputs, outputs=outputs)\n",
    "print(xformer.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xformer.compile(optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=[f1])\n",
    "\n",
    "history = xformer.fit(\n",
    "    X_train, y_train, batch_size=64, epochs=2,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_cnn = cnn.predict(X_test)\n",
    "yhat_xformer = xformer.predict(X_test)\n",
    "\n",
    "\n",
    "f1_scores = [mt.f1_score(y_test, np.round(yhat_cnn)),\n",
    "       mt.f1_score(y_test, np.round(yhat_xformer))]\n",
    "\n",
    "\n",
    "plt.bar([1,2],f1_scores)\n",
    "plt.xticks([1,2],['CNN','XFORMER'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Model 3: Modified CNN Sequential Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Model 4: Modified Transformer Sequential Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Adding Second Attention Layer to the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model Comparsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ConceptNet Numberbatch vs. GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
