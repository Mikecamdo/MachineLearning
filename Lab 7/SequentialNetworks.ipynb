{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Sequential Network Architectures\n",
    "\n",
    "## by Michael Doherty, Leilani Guzman, and Carson Pittman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"We've been trying to reach you about your car's extended warranty.\" There are few things more annoying in life than answering your phone and hearing these words. With technology being readily available to many people throughout the world, these types of scams have become much more prevalent. These \"spam\" messages, while not always scams, rarely add anything to society.\n",
    "\n",
    "While advancing technology has allowed spam messages to increase in numbers over the years, it has also allowed for new ways to combat these messages. These spam detection filters can automatically block any incoming calls, emails, texts, etc. that appear to be suspicious; however, these filters need to be fairly refined, as blocking too many legitimate messages would lead to distrust in the filter's reliability.\n",
    "\n",
    "Our dataset, titled \"SMS Spam Collection Dataset\", is comprised of over 5000 text messages in English, with each being tagged as either <code>spam</code> or <code>ham</code> (which means it's a legitimate message). Our task is to create a Sequential Network that can classify text messages as either <code>spam</code> or <code>ham</code>.\n",
    "\n",
    "Link to the dataset: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset\n",
    "\n",
    "## 1. Preparation\n",
    "### 1.1 Preprocessing and Tokenization\n",
    "To start, we'll first read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   v1          5572 non-null   object\n",
      " 1   v2          5572 non-null   object\n",
      " 2   Unnamed: 2  50 non-null     object\n",
      " 3   Unnamed: 3  12 non-null     object\n",
      " 4   Unnamed: 4  6 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/spam.csv\", encoding='ISO-8859-1')\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the dataset seems to include some useless columns... Let's go ahead and remove those. We'll also rename our remaining columns so their purpose is clearer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Label   5572 non-null   object\n",
      " 1   Text    5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(labels=[\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1, inplace=True)\n",
    "\n",
    "df.rename(columns={'v1': 'Label', 'v2': 'Text'}, inplace=True)\n",
    "\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset is looking better, let's tokenize the words in our <code>Text</code> variable. **ADD MORE ABOUT SPECIFICALLY WHAT WE DO TO TOKENIZE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary:\n",
      "8920\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10)\n",
    "tokenizer.fit_on_texts(df[\"Text\"])\n",
    "\n",
    "print(\"\\nVocabulary:\")\n",
    "print(len(tokenizer.word_index))\n",
    "# VOCAB_SIZE = len(tokenizer.word_index) \n",
    "# sequences = tokenizer.texts_to_sequences(df[\"Text\"])\n",
    "\n",
    "# padded_sequences = pad_sequences(sequences)\n",
    "max_review_length = 500\n",
    "X = tokenizer.texts_to_sequences(df[\"Text\"])\n",
    "X = pad_sequences(X, padding='post')\n",
    "y = df[\"Label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're done with preprocessing and tokenization, our final dataset can be described as follows:\n",
    "\n",
    "**INSERT DATASET DESCRIPTION (SIMILIAR TO LAB 5)**\n",
    "\n",
    "### 1.2 Performance Metric\n",
    "Now that we have our final dataset, let's visualize the distribution of values for our target variable <code>Label</code>; this will help us determine a suitable performance metric for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.countplot(x=df['Label'], palette='cividis')\n",
    "\n",
    "for bar in ax.patches:\n",
    "    ax.annotate(f'{bar.get_height():.0f}', (bar.get_x() + bar.get_width() / 2., bar.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 4), textcoords='offset points')\n",
    "\n",
    "plt.xticks([0,1], ['ham', 'spam'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "As seen above, there are significantly more messages labeled as <code>ham</code> than <code>spam</code>; thus, accuracy would not be a good metric (as it would be skewed towards the <code>ham</code> label). For this classification task, we care about both precision (making sure the messages we label as <code>spam</code> are actually spam) and recall (making sure all spam messages are labeled as <code>spam</code>). \n",
    "\n",
    "Precision is important, as we don't want to label legitimate messages as <code>spam</code> (as this would annoy users and make them distrust the legitimacy of our model); however, recall is also important, as we want to identify as many spam messages as possible. After all, a highly precise model that only identifies 1 out of 100 spam messages as actually being spam isn't ideal. Thus, we will use **F1 Score** as our evaluation metric, as it takes both precision and recall into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Training and Testing Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Now that we have our performance metric, we need to split our dataset into training and testing data. As we saw earlier, the labels in this dataset aren't balanced (as there are far more <code>ham</code> messages than <code>spam</code> messages). Thus, we will use StratifiedKFold to split our data into 5 splits for cross validation. StratifiedKFold ensures that each fold contains the same proportion of <code>ham</code> and <code>spam</code> messages. Furthermore, 5 splits allows for an 80/20 train test split (where our model trains on 80% of the data and uses the other 20% as validation data). Due to the large amount of data available to us, we believe this is sufficient.\n",
    "\n",
    "We believe this cross validation method is a realistic mirroring of how an algorithm would be used in practice, as this dataset is public information available for anyone to use. The cross validations splits would need to be stratified due to the class imbalance in the dataset, and 5 folds is sufficient, as an 80/20 train test split is a common practice in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "def get_f1_score(X, y, new_model):\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "    f1_scores = []\n",
    "    mean_tpr_list = []\n",
    "    auc_list = []\n",
    "\n",
    "    i = 1\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        X_train = pad_sequences(X_train, maxlen=max_review_length)\n",
    "        X_test = pad_sequences(X_test, maxlen=max_review_length)\n",
    "        print('Type of X_train:', type(X_train))\n",
    "        print('Type of X_test:', type(X_test))\n",
    "        print('Type of y_train:', type(y_train))\n",
    "        print('Type of y_test:', type(y_test))\n",
    "\n",
    "        VOCAB_SIZE = X_train.shape[1]\n",
    "        print(VOCAB_SIZE)\n",
    "        history = new_model.fit(X_train, y_train, batch_size=64,\n",
    "                                epochs=5, verbose=0,\n",
    "                                validation_data=(X_test,y_test),\n",
    "                                callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
    "        print('after fit')\n",
    "        print(f\"Fold {i}\")\n",
    "        i += 1\n",
    "        \n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(history.history['f1_score'], label='training')\n",
    "\n",
    "        plt.ylabel('F1 Score %')\n",
    "        plt.title('Training')\n",
    "        plt.plot(history.history['val_f1_score'], label='validation')\n",
    "        plt.title('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(history.history['loss'], label='training')\n",
    "        plt.ylabel('Training Loss')\n",
    "        plt.xlabel('epochs')\n",
    "\n",
    "        plt.plot(history.history['val_loss'], label='validation')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.title('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        f1_scores.append(history.history['val_f1_score'][-1].numpy())\n",
    "        print('HEREEEEEEEE')\n",
    "        yhat = new_model.predict(X_test, verbose=0)\n",
    "        yhat = tf.round(yhat).numpy()\n",
    "        print(type(yhat))\n",
    "\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(y_test, yhat.numpy())\n",
    "        mean_tpr_list.append(np.interp(mean_fpr, fpr, tpr))\n",
    "        auc_list.append(auc(fpr, tpr))\n",
    "\n",
    "    plt.bar(range(len(f1_scores)), f1_scores)\n",
    "    plt.ylim([min(f1_scores) - 0.01, max(f1_scores)])\n",
    "    plt.title('Validation F1 Score')\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('F1 Score')\n",
    "    print(\"Average F1 Score:\", np.mean(f1_scores))\n",
    "    \n",
    "    return f1_scores, mean_tpr_list, auc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling\n",
    "\n",
    "### 2.1 Model Creation\n",
    "\n",
    "#### 2.1.1 Model 1: CNN Sequential Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate\n",
    "from tensorflow.keras.layers import Subtract\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "NUM_CLASSES = 1\n",
    "EMBED_SIZE = 50\n",
    "# The input is a list of integers, 500 long\n",
    "# NEED TO DEFINE X_train\n",
    "sequence_input = Input(shape=(VOCAB_SIZE,), dtype='int32')\n",
    "\n",
    "# this will reduce the input dimension from VOCAB_SIZE to 50 for each word\n",
    "# the lenght will be the maximum number of words in a document, so 500\n",
    "embedded_sequences = Embedding(VOCAB_SIZE,\n",
    "                                EMBED_SIZE, # output dimension size\n",
    "                                input_length=max_review_length)(sequence_input) # number of words in each sequence\n",
    "\n",
    "# Starting sequence size is 500 (words) by 50 (embedded features)\n",
    "x = Conv1D(64, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(embedded_sequences)\n",
    "\n",
    "# After conv, length becomes: 500-4=496\n",
    "# So overall size is 496 by 64\n",
    "\n",
    "# Now pool across time\n",
    "x = MaxPooling1D(5)(x)# after max pool, 496/5 -> 99 by 64\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Extract additional features\n",
    "x = Conv1D(64, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(x)\n",
    "\n",
    "# New size is 95 after the conovlutions\n",
    "x = MaxPooling1D(5)(x) # after max pool, size is 95/5 = 19 by 64\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# More features through CNN processing\n",
    "x = Conv1D(64, 5, activation='relu',\n",
    "           kernel_initializer='he_uniform')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# After convolution, size becomes 15 elements long\n",
    "# Take the mean of these elements across features, result is 64 elements\n",
    "x_mean = GlobalAveragePooling1D()(x) # this is the size to globally flatten\n",
    "# Take the variance of these elements across features, result is 64 elements\n",
    "x_tmp = Subtract()([x,x_mean])\n",
    "x_std = GlobalAveragePooling1D()(x_tmp**2)\n",
    "\n",
    "x = Concatenate(name='concat_1')([x_mean,x_std])\n",
    "\n",
    "x = Dense(64, activation='relu',\n",
    "          kernel_initializer='he_uniform')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "predictions = Dense(NUM_CLASSES, activation='sigmoid',\n",
    "              kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "\n",
    "cnn = Model(sequence_input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "cnn.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[f1_score])\n",
    "\n",
    "print(type(X))\n",
    "y = np.array(y)\n",
    "print(type(y))\n",
    "f1_scores, mean_tpr_list, auc_list = get_f1_score(X, y, cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Summarize history for f1 score\n",
    "plt.plot(history[0].history['f1_score'])\n",
    "plt.plot(history[0].history['val_f1_score'])\n",
    "plt.title('CNN f1 score')\n",
    "plt.ylabel('f1 score')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# Summarize history for loss\n",
    "plt.plot(history[0].history['loss'])\n",
    "plt.plot(history[0].history['val_loss'])\n",
    "plt.title('CNN loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Model 2: Transformer Sequential Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "\n",
    "# The transformer architecture\n",
    "class TransformerBlock(Layer): # inherit from Keras Layer\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.2):\n",
    "        super().__init__()\n",
    "        # Setup the model heads and feedforward network\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embed_dim)\n",
    "       \n",
    "        # Make a two layer network that processes the attention\n",
    "        self.ffn = Sequential()\n",
    "        self.ffn.add( Dense(ff_dim, activation='relu') )\n",
    "        self.ffn.add( Dense(embed_dim) )\n",
    "       \n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Apply the layers as needed\n",
    "        # Get the attention output from multi heads\n",
    "        # Using same inpout here is self-attention\n",
    "        # Call inputs are (query, value, key)\n",
    "        # If only two inputs given, value and key are assumed the same\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "       \n",
    "        # Create residual output, with attention\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "       \n",
    "        # Apply dropout if training\n",
    "        out1 = self.dropout1(out1, training=training)\n",
    "       \n",
    "        # Place through feed forward after layer norm\n",
    "        ffn_output = self.ffn(out1)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "       \n",
    "        # Apply dropout if training\n",
    "        out2 = self.dropout2(out2, training=training)\n",
    "        #return the residual from Dense layer\n",
    "        return out2\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        # create two embeddings\n",
    "        # one for processing the tokens (words)\n",
    "        self.token_emb = Embedding(input_dim=vocab_size,\n",
    "                                   output_dim=embed_dim)\n",
    "        # another embedding for processing the position\n",
    "        self.pos_emb = Embedding(input_dim=maxlen,\n",
    "                                 output_dim=embed_dim)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        # create a static position measure (input)\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        # positions now goes from 0 to 500 by 1\n",
    "        positions = self.pos_emb(positions)# embed these positions\n",
    "        x = self.token_emb(x) # embed the tokens\n",
    "        return x + positions # add embeddngs to get final embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "x = TokenAndPositionEmbedding(X_train.shape[1], VOCAB_SIZE, embed_dim)(inputs)\n",
    "x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(20, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs = Dense(NUM_CLASSES, activation='sigmoid',\n",
    "              kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "\n",
    "xformer = Model(inputs=inputs, outputs=outputs)\n",
    "print(xformer.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xformer.compile(optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=[f1])\n",
    "\n",
    "history = xformer.fit(\n",
    "    X_train, y_train, batch_size=64, epochs=2,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_cnn = cnn.predict(X_test)\n",
    "yhat_xformer = xformer.predict(X_test)\n",
    "\n",
    "\n",
    "f1_scores = [mt.f1_score(y_test, np.round(yhat_cnn)),\n",
    "       mt.f1_score(y_test, np.round(yhat_xformer))]\n",
    "\n",
    "\n",
    "plt.bar([1,2],f1_scores)\n",
    "plt.xticks([1,2],['CNN','XFORMER'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Model 3: Modified CNN Sequential Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Model 4: Modified Transformer Sequential Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Adding Second Attention Layer to the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ConceptNet Numberbatch vs. GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
